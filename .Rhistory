n <- length(vec)
nT <- 0
for (i in 2:(n-1)){
if (((vec[i] > vec[i-1]) & (vec[i] > vec[i+1])) | ( (vec[i] < vec[i-1])
& (vec[i] < vec[i+1])))
{nT <- nT+1}
}
Tobs <- ( nT - 2*(n-2)/3 ) / sqrt( (16*n-29)/90 )
p <- 2 * (1- pnorm(abs(Tobs)))
res <- c(n,nT,Tobs,p)
names(res) <- c("n","nT","stat"," p-valeur")
return(res)
}
set.seed(12345)
y = rnorm(100)
PtTourn.test(y)
library(kernlab)
data(spam)
attach(spam)
dim(spam)
summary(spam)
apply(spam[,-58], 2, median)
apply(spam[,-58], 2, quantile, probs = 0,75)
library(reshape2)
library(ggplot2)
ggplot(data = melt(spam[, -58]), aes(variable, value)) +
geom_boxplot()
ggplot(data = spam, aes(x = type, y = you, fill = type)) +
geom_boxplot()
ggplot(data = spam, aes(x = type, y = credit, fill = type)) +
geom_boxplot()
#Paramètres de controle
ctrl <- trainControl(method = "none")
library(reshape2)
library(ggplot2)
ggplot(data = melt(spam[, -58]), aes(variable, value)) +
geom_boxplot()
ggplot(data = spam, aes(x = type, y = you, fill = type)) +
geom_boxplot()
ggplot(data = spam, aes(x = type, y = credit, fill = type)) +
geom_boxplot()
prop.table(table(type))
library(caret)
set.seed(100)
indxTrain <- createDataPartition(spam$type, p = 0.75, list = FALSE)
Dtrain <- spam[indxTrain,]
Dtest <- spam[-indxTrain,]
#Fréquence des classus sur l'échantillon d'apprentissage et test
print(prop.table(table(Dtrain$type)))
print(prop.table(table(Dtest$type)))
#Paramètres de controle
ctrl <- trainControl(method = "none")
#Apprentissage par régression logistique
fit.lr <- train(type ~ .,
data = Dtrain,
method = "glm",
trControl = ctrl)
print(fit.lr)
summary(fit.lr$finalModel)
print(varImp(fit.lr))
ctrl = trainControl("none")
fit.lr.aic = train(type ~ .,
data = Dtrain,
method = "glmStepAIC",
trControl = ctrl,
direction = "forward")
score.lr = predict(fit.lr.aic,
newdata = Dtest,
type = "prob")
tab = table(data = class.lr, reference = Dtest$type)
library(plotROC)
g = ggplot(score.lr,
aes(m = spam,
d = factor(Dtest$type, levels = c("nonspam","spam")))) +
geom_roc(n.cuts = 20) +
coord_equal() +
style_roc()
indxTrain = createDataPartition(spam$type, p = 0.75, list = FALSE)
Dsub = spam[indxTrain,]
Dtest = spam[-indxTrain,]
ctrl = trainControl(method = "none")
fit.knn = train(type ~ ., data = Dsub, method = "knn",
trControl = ctrl, tuneGrid = data.frame(k = 5),
preProcess = c("center","scale"))
fit.lr = train(type ~ ., data = Dsub, method = "glm",
trControl = ctrl, preProcess = c("center","scale"))
score.knn = predict(fit.knn, newdata = Dtest,type = "prob")
score.lr = predict(fit.lr, newdata = Dtest,type = "prob")
score.data = cbind(Dtest$type, score.knn["spam"], score.lr["spam"])
colnames(score.data) = c("type.test","knn","logit")
score.data <- melt_roc(score.data, "type.test", c("knn","logit"))
g=ggplot(score.data, aes(m = M,d = D,color = name)) + geom_roc()
g
#Précision des deux modèles
#On fait des classes pour pouvoir comparer les deux modèles
class.knn = predict(fit.knn, newdata = Dtest)
class.lr = predict(fit.lr, newdata = Dtest)
#On crée une table de confusion pour chaque modèle
tab.knn = table(data = class.knn, reference = Dtest$type)
tab.lr = table(data = class.lr, reference = Dtest$type)
#On crée une matrice de confusion pour chaque modèle
mat.knn = confusionMatrix(tab.knn, positive = "spam")
mat.lr = confusionMatrix(tab.lr, positive = "spam")
#On affiche les taux de vrais positifs et de vrais négatifs pour chaque modèle
mat.knn$byClass[c("Specificity","Sensitivity")]
mat.lr$byClass[c("Specificity","Sensitivity")]
#On affiche la specificity
mat.knn$overall["Accuracy"]
mat.lr$overall["Accuracy"]
#AUC
calc_auc(g)$AUC
#AUC
calc_auc(g)$AUC
#On met tous les résultats dans un même tableau
tabbb = matrix(c(mat.knn$overall["Accuracy"],mat.lr$overall["Accuracy"],mat.knn$byClass["Specificity"],mat.lr$byClass["Specificity"],mat.knn$byClass["Sensitivity"],mat.lr$byClass["Sensitivity"],calc_auc(g)$AUC),nrow = 4,ncol = 2)
View(tabbb)
#On met tous les résultats dans un même tableau avec les colonnes suivantes : modèle, AUC, précision, accuracy
tabb <- data.frame(model = c("knn","logit"),
AUC = c(calc_auc(g)$AUC,calc_auc(g.logit)$AUC),
precision = c(mat.knn$byClass["Specificity"],mat.lr$byClass["Specificity"]),
accuracy = c(mat.knn$overall["Accuracy"],mat.lr$overall["Accuracy"]))
#On met tous les résultats dans un même tableau avec les colonnes suivantes : modèle, AUC, précision, accuracy
tabb <- data.frame(model = c("knn","logit"),
AUC = c(calc_auc(g)$AUC,
precision = c(mat.knn$byClass["Specificity"],mat.lr$byClass["Specificity"]),
accuracy = c(mat.knn$overall["Accuracy"],mat.lr$overall["Accuracy"]))
#On met tous les résultats dans un même tableau avec les colonnes suivantes : modèle, AUC, précision, accuracy
tabb <- data.frame(model = c("knn","logit"),
AUC = calc_auc(g)$AUC,
precision = c(mat.knn$byClass["Specificity"],mat.lr$byClass["Specificity"]),
accuracy = c(mat.knn$overall["Accuracy"],mat.lr$overall["Accuracy"]))
View(tabb)
set.seed(100
)
indxTrain = createDataPartition(spam$type, p = 0.75, list = FALSE)
Dsub = spam[indxTrain,]
Dtest = spam[-indxTrain,]
ctrl = trainControl(method = "none")
fit.knn = train(type ~ ., data = Dsub, method = "knn",
trControl = ctrl, tuneGrid = data.frame(k = 5),
preProcess = c("center","scale"))
fit.lr = train(type ~ ., data = Dsub, method = "glm",
trControl = ctrl, preProcess = c("center","scale"))
score.knn = predict(fit.knn, newdata = Dtest,type = "prob")
score.lr = predict(fit.lr, newdata = Dtest,type = "prob")
score.data = cbind(Dtest$type, score.knn["spam"], score.lr["spam"])
colnames(score.data) = c("type.test","knn","logit")
score.data <- melt_roc(score.data, "type.test", c("knn","logit"))
g=ggplot(score.data, aes(m = M,d = D,color = name)) + geom_roc()
g
#Précision des deux modèles
#On fait des classes pour pouvoir comparer les deux modèles
class.knn = predict(fit.knn, newdata = Dtest)
class.lr = predict(fit.lr, newdata = Dtest)
#On crée une table de confusion pour chaque modèle
tab.knn = table(data = class.knn, reference = Dtest$type)
tab.lr = table(data = class.lr, reference = Dtest$type)
#On crée une matrice de confusion pour chaque modèle
mat.knn = confusionMatrix(tab.knn, positive = "spam")
mat.lr = confusionMatrix(tab.lr, positive = "spam")
#On affiche les taux de vrais positifs et de vrais négatifs pour chaque modèle
mat.knn$byClass[c("Specificity","Sensitivity")]
mat.lr$byClass[c("Specificity","Sensitivity")]
#On affiche la specificity
mat.knn$overall["Accuracy"]
mat.lr$overall["Accuracy"]
#AUC
calc_auc(g)$AUC
#On met tous les résultats dans un même tableau avec les colonnes suivantes : modèle, AUC, précision, accuracy
tabb <- data.frame(model = c("knn","logit"),
AUC = calc_auc(g)$AUC,
precision = c(mat.knn$byClass["Specificity"],mat.lr$byClass["Specificity"]),
accuracy = c(mat.knn$overall["Accuracy"],mat.lr$overall["Accuracy"]))
View(tabb)
#On met tous les résultats dans un même tableau avec les colonnes suivantes : modèle, AUC, précision, accuracy
#On utilise [f +- 1,96*sqrt(f*(1-f)/n)] pour calculer la marge d'erreur à chaque fois
tabb <- data.frame(model = c("knn","logit"),
AUC = c(0.944,0.944),
AUCmin = c(0.944-1.96*sqrt(0.944*(1-0.944)/4601),0.944-1.96*sqrt(0.944*(1-0.944)/4601)),
AUCmax = c(0.944+1.96*sqrt(0.944*(1-0.944)/4601),0.944+1.96*sqrt(0.944*(1-0.944)/4601)),
precision = c(0.945,0.945),
precisionmin = c(0.945-1.96*sqrt(0.945*(1-0.945)/4601),0.945-1.96*sqrt(0.945*(1-0.945)/4601)),
precisionmax = c(0.945+1.96*sqrt(0.945*(1-0.945)/4601),0.945+1.96*sqrt(0.945*(1-0.945)/4601)),
accuracy = c(0.925,0.925),
accuracymin = c(0.925-1.96*sqrt(0.925*(1-0.925)/4601),0.925-1.96*sqrt(0.925*(1-0.925)/4601)),
accuracymax = c(0.925+1.96*sqrt(0.925*(1-0.925)/4601),0.925+1.96*sqrt(0.925*(1-0.925)/4601)))
#On met tous les résultats dans un même tableau avec les colonnes suivantes : modèle, AUC, précision, accuracy
tabb <- data.frame(model = c("knn","logit"),
AUC = calc_auc(g)$AUC,
precision = c(mat.knn$byClass["Specificity"],mat.lr$byClass["Specificity"]),
accuracy = c(mat.knn$overall["Accuracy"],mat.lr$overall["Accuracy"]))
View(tabb)
#On met tous les résultats dans un même tableau avec les colonnes suivantes : modèle, AUC, précision, accuracy
tabb <- data.frame(model = c("knn","logit"),
AUC = calc_auc(g)$AUC,
"precision/spécifisity" = c(mat.knn$byClass["Specificity"],mat.lr$byClass["Specificity"]),
accuracy = c(mat.knn$overall["Accuracy"],mat.lr$overall["Accuracy"]))
#On met tous les résultats dans un même tableau avec les colonnes suivantes : modèle, AUC, précision, accuracy
tabb <- data.frame(model = c("knn","logit"),
AUC = calc_auc(g)$AUC,
"precision/specifisity" = c(mat.knn$byClass["Specificity"],mat.lr$byClass["Specificity"]),
accuracy = c(mat.knn$overall["Accuracy"],mat.lr$overall["Accuracy"]))
#On met tous les résultats dans un même tableau avec les colonnes suivantes : modèle, AUC, précision, accuracy
tabb <- data.frame(model = c("knn","logit"),
AUC = calc_auc(g)$AUC,
precision = c(mat.knn$byClass["Specificity"],mat.lr$byClass["Specificity"]),
accuracy = c(mat.knn$overall["Accuracy"],mat.lr$overall["Accuracy"]))
#On met tous les résultats dans un même tableau avec les colonnes suivantes : modèle, AUC, précision, accuracy
tabb <- data.frame(model = c("knn","logit"),
AUC = calc_auc(g)$AUC,
precision = c(mat.knn$byClass["Specificity"],mat.lr$byClass["Specificity"]),
accuracy = c(mat.knn$overall["Accuracy"],mat.lr$overall["Accuracy"]))
#On refait le même tableau mais avec un interval [x,x] avec plus et moins 1,96*sqrt(f*(1-f)/n)
e <- 1.96*sqrt(tabb$precision*(1-tabb$precision)/4601)
tabb <- cbind(tabb, e)
colnames(tabb) <- c("model","AUC","precision","accuracy","e")
tabb
#On met tous les résultats dans un même tableau avec les colonnes suivantes : modèle, AUC, précision, accuracy
tabb <- data.frame(model = c("knn","logit"),
AUC = calc_auc(g)$AUC,
precision = c(mat.knn$byClass["Specificity"],mat.lr$byClass["Specificity"]),
accuracy = c(mat.knn$overall["Accuracy"],mat.lr$overall["Accuracy"]))
#AUC
calc_auc(g)$AUC
??nrow
??nrows
help(nrows)
help(nrow)
11 % 6
library(leaflet)
m <- leaflet() %>%
addTiles() %>%  # Add default OpenStreetMap map tiles
addMarkers(lng=174.768, lat=-36.852, popup="The birthplace of R")
m  # Print the map
devtools::install_github("tylermorganwall/rayshader")
install.packages("rayshader")
elmat %>%
sphere_shade(texture = "desert") %>%
add_water(detect_water(elmat), color = "lightblue") %>%
add_shadow(cloud_shade(elmat,zscale = 10, start_altitude = 500, end_altitude = 700,
sun_altitude = 45, attenuation_coef = 2, offset_y = 300,
cloud_cover = 0.55, frequency = 0.01, scale_y=3, fractal_levels = 32), 0) %>%
plot_3d(elmat, zscale = 10, fov = 0, theta = 135, zoom = 0.75, phi = 45, windowsize = c(1000, 800),
background="darkred")
library(rayshader)
elmat %>%
sphere_shade(texture = "desert") %>%
add_water(detect_water(elmat), color = "lightblue") %>%
add_shadow(cloud_shade(elmat,zscale = 10, start_altitude = 500, end_altitude = 700,
sun_altitude = 45, attenuation_coef = 2, offset_y = 300,
cloud_cover = 0.55, frequency = 0.01, scale_y=3, fractal_levels = 32), 0) %>%
plot_3d(elmat, zscale = 10, fov = 0, theta = 135, zoom = 0.75, phi = 45, windowsize = c(1000, 800),
background="darkred")
(table_contingence <- matrix(c(15, 25, 61, 84), nrow = 2, ncol = 2, byrow = TRUE))
(odds_ratio <- (table_contingence[1, 1] / table_contingence[1, 2]) / (table_contingence[2, 1] / table_contingence[2, 2]))
install.packages("furrr")
install.packages("future.apply")
rstudioapi::addTheme("https://raw.githubusercontent.com/dracula/rstudio/master/dracula.rstheme", apply = TRUE)
rstudioapi::addTheme("https://raw.githubusercontent.com/dracula/rstudio/master/dracula.rstheme", apply = TRUE)
remotes::install_github("anthonynorth/rscodeio")
rscodeio::install_theme()
rscodeio::install_theme()
remotes::install_github("anthonynorth/rscodeio")
rscodeio::install_theme()
activate_menu_theme()
library(rscodeio)
activate_menu_theme()
install.packages("rgl")  # Installez le package rgl si ce n'est pas déjà fait
library(rgl)
# Paramètres pour le tore
radius_outer = 2   # Rayon externe
radius_inner = 0.5 # Rayon interne
divs = 100         # Nombre de divisions pour le rendu
# Créer le tore
open3d()   # Ouvre une fenêtre de visualisation 3D
rgl.torus(radius_outer, radius_inner, divs)
install.packages("rgl")
library(rgl)
# Paramètres du tore
radius_outer = 2   # Rayon externe
radius_inner = 0.5 # Rayon interne
divs = 100         # Nombre de divisions pour le rendu
# Fonction pour calculer les coordonnées du tore
torusCoords <- function(outer_radius, inner_radius, divs) {
theta <- seq(0, 2 * pi, length.out = divs)
phi <- seq(0, 2 * pi, length.out = divs)
coords <- expand.grid(theta = theta, phi = phi)
x <- (outer_radius + inner_radius * cos(coords$theta)) * cos(coords$phi)
y <- (outer_radius + inner_radius * cos(coords$theta)) * sin(coords$phi)
z <- inner_radius * sin(coords$theta)
return(data.frame(x, y, z))
}
# Créer et afficher le tore
coords <- torusCoords(radius_outer, radius_inner, divs)
plot3d(coords$x, coords$y, coords$z, type = "s", size = 0.1)
install.packages("rgl")
install.packages("rgl")
library(rgl)
# Paramètres du tore
radius_outer = 2   # Rayon externe
radius_inner = 0.5 # Rayon interne
divs = 100         # Nombre de divisions pour le rendu
# Fonction pour calculer les coordonnées du tore
torusCoords <- function(outer_radius, inner_radius, divs) {
theta <- seq(0, 2 * pi, length.out = divs)
phi <- seq(0, 2 * pi, length.out = divs)
coords <- expand.grid(theta = theta, phi = phi)
x <- (outer_radius + inner_radius * cos(coords$theta)) * cos(coords$phi)
y <- (outer_radius + inner_radius * cos(coords$theta)) * sin(coords$phi)
z <- inner_radius * sin(coords$theta)
return(data.frame(x, y, z))
}
# Créer et afficher le tore
coords <- torusCoords(radius_outer, radius_inner, divs)
plot3d(coords$x, coords$y, coords$z, type = "s", size = 0.1)
install.packages("kernlab")
library(tseries)
install.packages("sarima")
library(sarima)
data(bev)
bev
data(ice.river)
ice.river
#Pour simuler un sarima on utilise la fonction sim_sarima
#La fonction sarima sert à faire l'estimation
x <- sim_sarima(n=144, model = list(ar=c(1.2,-0.8), ma=0.4,
sar=0.3, sma=0.7, iorder=1, siorder=1,
nseasons=12, sigma2 = 1))
knitr::opts_chunk$set(echo = TRUE)
library(sarima)
setwd("C:/Users/thoma/Desktop/Github/Web-page-Phishing-Detection")
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(caret)
library(tidyr)
library(reshape2)
library(plotROC)
library(ROCR)
df <- read_csv("dataset_phishing.csv", show_col_types = FALSE)
df <- df[,-1]
df$status <- as.factor(df$status)
df_present <- df
#Extraire les variables qualitatives
v_quali <- vector("logical", length = ncol(df_present) - 1)
for (i in 2:ncol(df_present)) {
v_quali[[i]] <- (length(unique(df_present[[i]])) / sum(!is.na(df_present[[i]]))) < 0.002
}
num_cols <- character()
cat_cols <- character()
for (i in 1:length(v_quali)) {
if (!v_quali[[i]]) {
num_cols <- c(num_cols, names(df_present)[i])
} else {
cat_cols <- c(cat_cols, names(df_present)[i])
}
}
corr <- cor(df_present[num_cols])
ggcorrplot(
corr,
hc.order = TRUE,
type = "full",
outline.color = "white",
ggtheme = ggplot2::theme_gray,
colors = c("#6D9EC1", "white", "#E46726"),
show.diag = TRUE,
tl.cex = 7,
tl.srt = 90
)
ggplot(data = melt(df_present[, num_cols]), aes(x = variable, y = value)) +
geom_boxplot() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
attach(df)
ggplot(df, aes(x = log(length_url), y = log(domain_age))) +
geom_point() +
labs(x = "Longueur de l'URL", y = "Âge du domaine") +
ggtitle("Nuage de points : Longueur de l'URL vs Âge du domaine")
max(domain_age)
min(domain_age)
# Nuage de point y = longueur url, x = status
ggplot(df, aes(x = status, y = length_url)) +
geom_point() +
labs(x = "Status", y = "Longueur de l'URL") +
ggtitle("Nuage de points : Longueur de l'URL vs Status")
# Boxplot
ggplot(df, aes(x = status, y = log(length_url))) +
geom_boxplot() +
labs(x = "Status", y = "Longueur de l'URL") +
ggtitle("Boxplot : Longueur de l'URL vs Status")
mean_by_status <- function(df, col_name) {
df %>%
group_by(status) %>%
summarise(mean_value = mean(.data[[col_name]], na.rm = TRUE))
}
mean_values_list_cat <- list()
for (col in cat_cols) {
mean_values_list_cat[[col]] <- mean_by_status(df_present, col)
}
mean_values_df_cat <- do.call(rbind, mean_values_list_cat)
mean_values_df_cat$col_names <- rownames(mean_values_df_cat)
mean_values_df_cat <- mean_values_df_cat[mean_values_df_cat$mean_value > 0.1 | mean_values_df_cat$mean_value < -0.1,]
mean_values_df_cat <- mean_values_df_cat[!is.na(mean_values_df_cat$mean_value), ]
ggplot(mean_values_df_cat, aes(x = col_names, y = mean_value, fill = status)) +
geom_bar(stat = "identity", position = "dodge") +
labs(x = "Variables", y = "Moyenne", title = "Moyenne des variables qualitatives par statut") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_manual(values = c("#E46726", "#6D9EC1"))
mean_values_list_num <- list()
for (col in num_cols) {
if (col != "web_traffic" && col != "domain_age" && col != "domain_registration_length") {
mean_values_list_num[[col]] <- mean_by_status(df_present, col)
}
}
mean_values_df_num <- do.call(rbind, mean_values_list_num)
mean_values_df_num$col_names <- rownames(mean_values_df_num)
mean_values_df_num <- mean_values_df_num[mean_values_df_num$mean_value > 0.3 | mean_values_df_num$mean_value < -0.3,]
mean_values_df_num <- mean_values_df_num[!is.na(mean_values_df_num$mean_value), ]
ggplot(mean_values_df_num, aes(x = col_names, y = mean_value, fill = status)) +
geom_bar(stat = "identity", position = "dodge") +
labs(x = "Variables", y = "Moyenne", title = "Moyenne des variables quantitatives par statut") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_manual(values = c("#E46726", "#6D9EC1"))
set.seed(123)
indxTrain <- createDataPartition(df$status, p = 0.75, list = FALSE)
DTrain <- df[indxTrain, ]
DTest <- df[-indxTrain, ]
cat("Nombre d'observations dans l'ensemble d'entraînement:", nrow(DTrain), "\n")
cat("Nombre d'observations dans l'ensemble de test:", nrow(DTest), "\n")
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)
k_values <- c(1:10, seq(12, 19, by = 2), seq(20, 99, by = 5), seq(100, 500, by = 50))
tune_grid <- expand.grid(k = k_values)
fit.knn.cv <- train(
status ~ .,
data = DTrain,
method = "knn",
trControl = ctrl,
tuneGrid = tune_grid,
preProcess = c("center", "scale"),
na.action = na.omit
)
plot(fit.knn.cv)
print(fit.knn.cv$results)
print(fit.knn.cv$bestTune)
bestK <- fit.knn.cv$bestTune$k
predictionsBestK <- predict(fit.knn.cv, newdata = DTest)
confusionMatrixBestK <- confusionMatrix(predictionsBestK, DTest$status)
errorRateBestK <- 1 - confusionMatrixBestK$overall['Accuracy']
print(errorRateBestK)
set.seed(123)
ctrl = trainControl(method = "cv",
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "all" )
fit.lr <- train(status ~ .,
data = DTrain,
method = "glm",
trControl = ctrl,
na.action = na.omit)
class.lr <- predict(fit.lr, newdata = DTest, type = "prob")
print(varImp(fit.lr))
ctrl = trainControl(method = "cv",
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "all" )
fit.lr.aic <- train(status ~ .,
data = DTrain,
method = "glmStepAIC",
trControl = ctrl,
na.action = na.omit)
fit.lda <- train(status ~ .,
data = DTrain,
method = "lda",
trControl = ctrl)
fit.lda <- train(status ~ .,
data = DTrain,
method = "lda",
trControl = ctrl)
set.seed(123)
ctrl = trainControl(method = "cv",
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "all" )
fit.lda <- train(status ~ .,
data = DTrain,
method = "lda",
trControl = ctrl,
na.action = na.omit)
View(df)
View(df)
set.seed(123)
ctrl = trainControl(method = "cv", number = 10)
fit.nb <- train(status ~ .,
data = DTrain,
method = "lda",
trControl = ctrl,
na.action = na.omit)
set.seed(123)
ctrl = trainControl(method = "cv", number = 10)
fit.nb <- train(status ~ .,
data = DTrain,
method = "lda",
trControl = ctrl)
set.seed(123)
ctrl = trainControl(method = "cv", number = 10)
fit.nb <- train(status ~ .,
data = DTrain,
method = "lda",
trControl = ctrl)
