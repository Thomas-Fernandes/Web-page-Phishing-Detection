---
title: "Web page Phishing Detection project"
output: 
  html_document:
    code_folding: show
    theme:
      bg: "#202123"
      fg: "#B8BCC2"
      primary: "#EA80FC"
      base_font:
        google: Prompt
      heading_font:
        google: Proza Libre
      version: 3
---

```{r setup, include=FALSE}
if (requireNamespace("thematic")) 
  thematic::thematic_rmd(font = "auto")
```

```{r, include=FALSE, warning=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(caret)

df <- read_csv("dataset_phishing.csv", show_col_types = FALSE)
df <- df[,-1]
```

## {.tabset .tabset-pills}

### Présentation des données

Le phishing est une méthode de cyberfraude courante et efficace, exploitée par les cybercriminels pour dérober des informations personnelles et financières. Avec l'augmentation de notre dépendance à l'internet pour nos activités quotidiennes, les attaques de phishing sont devenues plus sophistiquées et difficiles à identifier. Une étude d'Intel a révélé que 97 % des experts en sécurité échouent à distinguer les courriels de phishing des courriels authentiques.

Contenu du Jeu de Données

Le jeu de données fourni contient 11 430 URL, chacune décrite par 87 caractéristiques distinctes. Ces données sont conçues pour servir de référence aux systèmes de détection de phishing basés sur l'apprentissage automatique. Les caractéristiques sont divisées en trois classes :

- 56 basées sur la structure et la syntaxe des URL,
- 24 extraites du contenu des pages web correspondantes,
- 7 obtenues par des requêtes auprès de services externes.

Le jeu de données est équilibré, composé à 50 % d'URL de phishing et à 50 % d'URL légitimes, ce qui est idéal pour l'entraînement de modèles de classification.

#### 1. Visualisation du jeu de données

```{r}
head(df)
```
#### 2. Distribution des classes

```{r}
df$status <- as.factor(df$status)
ggplot(df, aes(x=status, fill=status)) +
  geom_bar() +
  labs(x="Classe", y="Nombre d'Observations", fill="Statut") +
  ggtitle("Distribution des Classes de Phishing vs Légitimes")
```
#### 3. Distribution des caractéristiques

```{r}
# Histogramme pour 'length_url'
ggplot(df, aes(x=length_url)) +
  geom_histogram(bins=30, fill="blue", alpha=0.7) +
  labs(x="Longueur de l'URL", y="Fréquence") +
  ggtitle("Distribution de la Longueur des URL") +
  theme_minimal()

# Boîte à moustaches pour 'length_url'
ggplot(df, aes(y=length_url, x=status, fill=status)) +
  geom_boxplot() +
  labs(x="Statut", y="Longueur de l'URL", fill="Statut") +
  ggtitle("Boîte à Moustaches de la Longueur des URL par Statut") +
  theme_minimal()

# Histogramme pour 'nb_hyphens'
ggplot(df, aes(x=nb_hyphens)) +
  geom_histogram(bins=30, fill="green", alpha=0.7) +
  labs(x="Nombre de Tirets", y="Fréquence") +
  ggtitle("Distribution du Nombre de Tirets dans les URL") +
  theme_minimal()

# Histogramme pour 'ratio_digits_url'
ggplot(df, aes(x=ratio_digits_url)) +
  geom_histogram(bins=30, fill="red", alpha=0.7) +
  labs(x="Ratio de Chiffres dans l'URL", y="Fréquence") +
  ggtitle("Distribution du Ratio de Chiffres dans les URL") +
  theme_minimal()

```
#### 4. Corrélation des caractéristiques

```{r}
numeric_columns <- sapply(df, is.numeric)
cor_matrix <- cor(df[, numeric_columns], use = "complete.obs")
corrplot(cor_matrix, method = "circle", tl.cex = 0.6, cl.cex = 0.6)
```


### Préparation des données

```{r}
indxTrain <- createDataPartition(df$status, p = 0.75, list = FALSE)
DTrain <- df[indxTrain, ]
DTest <- df[-indxTrain, ]

cat("Nombre d'observations dans l'ensemble d'entraînement:", nrow(DTrain), "\n")
cat("Nombre d'observations dans l'ensemble de test:", nrow(DTest), "\n")
```
Test

### K-NN

# 1. prédictions

```{r}
ctrl <- trainControl(method = "none")
set.seed(123) # Pour la reproductibilité
fit.knn <- train(status ~ ., data = DTrain, method = "knn", tuneGrid = data.frame(k = 5), trControl = ctrl)
predictions <- predict(fit.knn, newdata = DTest)
```

# 2. évaluation du modèle

Dans cette partie de l'analyse, nous évaluons la performance du modèle des k-plus proches voisins (K-NN) que nous avons entraîné pour la détection de sites de phishing. L'utilisation d'une matrice de confusion nous permet de comparer les prédictions du modèle par rapport aux valeurs réelles. La matrice de confusion est un outil puissant pour mesurer la précision, la sensibilité, la spécificité et d'autres métriques importantes de la performance du modèle.

```{r}
# Matrice de confusion
confusionMatrix <- confusionMatrix(predictions, DTest$status)

print(confusionMatrix$table)
print(confusionMatrix$overall['Accuracy'])

errorRate <- 1 - confusionMatrix$overall['Accuracy']
print(errorRate)

```
D'après les résultats obtenus, le modèle a une précision d'environ 84.17%, ce qui signifie qu'il a correctement prédit 84.17% des URL comme étant légitimes ou de phishing. La matrice montre également les répartitions spécifiques des vrais positifs, vrais négatifs, faux positifs et faux négatifs. Plus précisément, le modèle a correctement identifié 1,187 URL légitimes et 1,217 URL de phishing, tandis qu'il a incorrectement classé 211 URL légitimes comme phishing et 241 URL de phishing comme légitimes. Le taux d'erreur de 15.82% reflète la proportion de prédictions incorrectes par rapport au total des prédictions.

# 3. Choix du K

Dans cette section de votre analyse, vous avez effectué une recherche du nombre optimal de voisins k pour le modèle des k-plus proches voisins (K-NN) à l'aide de la validation croisée 10-fold. Cela implique de diviser l'ensemble de données d'entraînement en 10 parties, d'utiliser 9 d'entre elles pour l'entraînement et une pour la validation, et de répéter ce processus 10 fois avec des parties différentes à chaque fois pour la validation.

```{r, warning=FALSE}
# Recherche du meilleur k
ctrl <- trainControl(method = "cv", number = 10) # 10-fold cross-validation

# Entraînement du modèle avec une recherche de différents k
knnFit <- train(status ~ ., data = DTrain, method = "knn", trControl = ctrl, tuneLength = 20, preProcess = c("center", "scale"))

# Visualisation des résultats
plot(knnFit)
print(knnFit$results)
print(knnFit$bestTune)

# Estimation de l'erreur de prédiction avec le meilleur k sur les données de test
bestK <- knnFit$bestTune$k
predictionsBestK <- predict(knnFit, newdata = DTest)
confusionMatrixBestK <- confusionMatrix(predictionsBestK, DTest$status)
errorRateBestK <- 1 - confusionMatrixBestK$overall['Accuracy']
print(errorRateBestK)
```
Le graphique généré illustre comment la précision de la validation croisée varie en fonction du nombre de voisins k. D'après la visualisation, il semble que la précision augmente lorsque le nombre de voisins est faible et diminue après avoir atteint un pic. Cela suggère qu'un nombre plus réduit de voisins aide le modèle à mieux capturer les nuances des données sans tomber dans le surajustement, où le modèle est trop spécifique aux données d'entraînement et ne généralise pas bien aux nouvelles données.

En analysant les données, j'observe que la précision la plus élevée est obtenue avec k=5. Cela suggère que le modèle classifie les données avec le plus de précision lorsqu'il considère les 5 voisins les plus proches. Ensuite, bien que la précision diminue légèrement avec k=7, elle reste relativement élevée pour k allant jusqu'à 13, après quoi la précision commence à diminuer de manière plus significative. Cela peut indiquer que des valeurs de k plus faibles sont préférables pour ce dataset spécifique, mais qu'il existe une marge avant que l'augmentation de k n'entraîne un sous-ajustement notable.

Il est également important de noter les écarts-types des précisions (AccuracySD) et des scores Kappa (KappaSD), qui fournissent une indication de la variabilité de la performance du modèle à travers les différentes itérations de la validation croisée. Des écarts-types plus faibles sont préférables car ils impliquent une performance plus constante du modèle.

### Régression logistique
